# 16 强化学习

## 16.1 任务与背景

多步决策过程

过程中包含状态、动作、反馈（奖赏）等

强化学习常用**马尔可夫决策过程**描述

- 机器所处的环境 E
- 状态空间 $\{X{:}x\in X\}$ 是机器感知到的环境的描述
- 智能体能采取的行为空间 $A$
- 潜在的状态转移（概率）函数 $P{:}X\times A\times X\to\mathbb{R}$
- 潜在的奖赏（reward）函数 $R{: }X\times A\times X\to \mathbb{R} \left ( \text{或  }R{: }X\times X\to \mathbb{R} \right )$
- 策略（policy）$\pi{:}X\to A$（或 $\pi{:}X\times A\to\mathbb{R}$）

机器通过在环境中不断尝试从而学到一个策略 $\pi$，使得长期执行该策略后得到的期望累积奖赏最大

$T$ 步期望累积奖赏：$\mathbb{E} [ \frac 1T\sum _{t= 1}^Tr_t]$

$\gamma$ 期望折扣累积奖赏：$\mathbb{E}[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}]$


$a= \pi ( x)$ $< x_{0}, a_{0}, r_{1}, x_{1}, a_{1}, r_{2}, \ldots , x_{T- 1}, a_{T- 1}, r_{T}, x_{T}>$

最大化 $\mathbb{E}[\frac1T\sum_{t=1}^Tr_t]$

## 16.2 K - 摇臂赌博机

### 16.2.1 探索与利用

K-摇臂赌博机只有一个状态，K 个动作，每个摇臂的奖赏服从某个期望未知的分布，目标是执行有限次动作，最大化累积奖赏

强化学习面临的主要困难：探索-利用窘境

- 仅探索：估计不同摇臂的优劣（奖赏期望的大小）
- 仅利用：选择当前最优的摇臂

### 16.2.2 $𝜖$ - 贪心

- 以 $\epsilon$ 的概率探索：均匀随机选择一个摇臂
- 以 $1-\epsilon$ 的概率利用：选择当前平均奖赏最高的摇臂

### 16.2.3 Softmax

基于当前已知的摇臂平均奖赏来对探索与利用折中

- 若某个摇臂当前的平均奖赏越大，则它被选择的概率越高

- 概率分配使用 Boltzmann 分布：
  $$
  P(k)=\frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^{K}e^{\frac{Q(i)}{\tau}}}
  $$
  $Q(i)$ 记录当前摇臂的平均奖赏



## 16.3 有模型学习

有模型学习：E=<X, A, P, R>，X, A, P, R 均已知，假设状态空间和动作空间均有限

强化学习的目标：找到使累积奖赏最大的策略 $\pi$

### 16.3.1 策略评估

使用某策略所带来的累积奖赏

- 状态值函数：从状态 x 出发，使用策略 $\pi$ 所带来的累积奖赏
  $$
  \begin{cases}V_T^\pi(x)=\mathbb{E}_\pi\left[\frac{1}{T}\sum_{t=1}^Tr_t\mid x_0=x\right],&T\text{ 步累积奖赏} \\
  V_\gamma^\pi(x)=\mathbb{E}_\pi\left[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}\mid x_0=x\right],&\gamma\text{ 折扣累积奖赏}&
  \end{cases}
  $$
  给定 $\pi$，值函数的计算：值函数具有简单的递归形式

  - T 步累积奖赏：
    $$
    \begin{aligned}
    V_{T}^{\pi}(x)&=\mathbb{E}_{\pi}\left[\frac{1}{T}\sum_{t=1}^Tr_t\mid x_0=x\right]\\&=\mathbb{E}_\pi\left[\frac{1}{T}r_1+\frac{T-1}{T}\frac{1}{T-1}\sum_{t=2}^Tr_t\mid x_0=x\right]\\&=\sum_{a\in A}\pi(x,a)\sum_{x^{\prime}\in X}P_{x\to x^{\prime}}^a\left(\frac{1}{T}R_{x\to x^{\prime}}^a+\frac{T-1}{T}\mathbb{E}_\pi\left[\frac{1}{T-1}\sum_{t=1}^{T-1}r_t\mid x_0=x^{\prime}\right]\right)\\&=\sum_{a\in A}\pi(x,a)\sum_{x^{\prime}\in X}P_{x\to x^{\prime}}^a\left(\frac{1}{T}R_{x\to x^{\prime}}^a+\frac{T-1}{T}V_{T-1}^\pi(x^{\prime})\right)
    \end{aligned}
    $$

  - 折扣累积奖赏：
    $$
    V_\gamma^\pi(x)=\sum_{a\in A}\pi(x,a)\sum_{x^{\prime}\in X}P_{x\to x^{\prime}}^a\left(R_{x\to x^{\prime}}^a+\gamma V_\gamma^\pi(x^{\prime})\right)
    $$

- 状态-动作值函数：从状态 x 出发，执行动作 a 后再使用策略 $\pi$ 所带来的累积奖赏
  $$
  \begin{cases}Q_T^\pi(x,a)=\mathbb{E}_\pi[\frac{1}{T}\sum_{t=1}^Tr_t\mid x_0=x,a_0=a]\\Q_\gamma^\pi(x,a)=\mathbb{E}_\pi[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}\mid x_0=x,a_0=a]&\end{cases}
  $$
  给定 $\pi$，状态-动作值函数的计算：通过值函数来表示

  最优策略，最优值函数，最优状态-动作值函数

  - 最优策略：最大化累积奖赏 $\pi^*=\underset{\pi}{\operatorname*{\operatorname*{argmax}}}\sum_{x\in X}V^\pi(x)$
  - 最优值函数：$\forall x\in X:V^*(x)=V^{\pi^*}(x)$

  - 最优状态-动作值函数

评估一个策略的值函数：$V^\pi(x)=R(x,\pi(x))+\gamma\sum_{x^{\prime}}P_{x\to x^{\prime}}V^\pi\left(x^{\prime}\right)$

求解方法：$V_t^{\boldsymbol{\pi}}(x)=R\left(x,\pi(x)\right)+\gamma\sum_{x^{\prime}}P_{x\to x^{\prime}}V_{t-1}(x^{\prime})$

通过这种方式得到的值函数收敛到正确的值函数
$$
\begin{aligned}
\|V_t^\pi(x)-V^\pi(x)\|_\infty&=\max_x\gamma\sum_{x^{\prime}}P_{x\to x^{\prime}}|V_{t-1}^\pi(x^{\prime})-V^\pi(x^{\prime})|\\
&\leq\gamma\sum_{x^{\prime}}P_{x\to x^{\prime}}\max|V_{t-1}^\pi(x^{\prime})-V^\pi(x^{\prime})|\\&=\gamma\max|V_{t-1}^\pi(x^{\prime})-V^\pi(x^{\prime})|\\&=\gamma\|V_{t-1}^\pi(x)-V^\pi(x)\|_\infty\end{aligned}
$$

### 16.3.2 策略改进

将非最优策略改进为最优策略

最优值函数/最优策略满足：
$$
\begin{cases}V_{T}^{*}(x)=\max_{a\in A}\sum_{x^{\prime}\in X}P_{x\to x^{\prime}}^{a}\left(\frac{1}{T}R_{x\to x^{\prime}}^{a}+\frac{T-1}{T}V_{T-1}^{*}(x^{\prime})\right)\\V_{\gamma}^{*}(x)=\max_{a\in A}\sum_{x^{\prime}\in X}P_{x\to x^{\prime}}^{a}\left(R_{x\to x^{\prime}}^{a}+\gamma V_{\gamma}^{*}(x^{\prime})\right)
\end{cases}
$$

$$
V^{*}(x)=\max_{a\in A}Q^{\pi^{*}}(x,a)
$$

非最优策略的改进方式：将策略选择的动作改为当前最优的动作 $\pi'(x)=\underset{a\in A}{\operatorname*{argmax}}Q^\pi(x,a)$

### 16.3.3 策略迭代与值迭代

策略迭代：求解最优策略的方法



## 16.4 免模型学习

转移概率、奖赏函数未知，环境中的状态数目也未知，假定状态空间有限

### 16.4.1 蒙特卡罗强化学习

采样轨迹，用样本均值近似期望

- 策略评估：蒙特卡罗法

  - 从某状态出发，执行某策略

  - 对轨迹中出现的每对状态-动作，记录其后的奖赏之和

  - 采样多条轨迹，每个状态-动作对的累积奖赏取平均

- 策略改进：换入当前最优动作

可能遇到的问题：轨迹的单一性。解决方法：$𝜖$ - 贪心

- 同策略：被评估与被改进的是同一个策略
- 异策略：被评估与被改进的是不同的策略

缺点：低效，在一个完整的采样轨迹完成后才对状态-动作值函数进行更新

### 16.4.2 时序差分学习

增量式地进行状态-动作值函数更新

$𝜖$ - 贪心

- 同策略：Sarsa算法
- 异策略：Q-学习



## 16.5 值函数近似

状态空间连续，值函数近似，将值函数表达为状态的线性函数，用最小二乘误差来度量学到的值函数与真实的值函数之间的近似程度，用梯度下降法更新参数向量，求解优化问题



## 16.6 模仿学习

直接模仿人类专家的状态-动作对来学习策略

### 16.6.1 直接模仿学习

- 利用专家的决策轨迹，构造数据集 $D$：状态作为特征，动作作为标记
- 利用数据集 $D$，使用分类/回归算法即可学得策略
- 将学得的策略作为初始策略
- 策略改进，从而获得更好的策略

### 16.6.2 逆强化学习

寻找某种奖赏函数使得范例数据是最优的，然后即可使用这个奖赏函数来训练强化学习策略