# 10 降维与度量学习

## 10.1 k 近邻学习

确定训练样本，以及某种距离度量。

对于某个给定的测试样本，找到训练集中距离最近的 k 个样本：

- 对于分类问题使用“投票法”获得预测结果：选择这 k 个样本中出现最多的类别标记作为预测结果
- 对于回归问题使用“平均法”获得预测结果：将这 k 个样本的实值输出标记的平均值作为预测结果
- 还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大

懒惰学习：在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理

急切学习：在训练阶段就对样本进行学习处理



## 10.2 低维嵌入

在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”

降维：通过某种数学变换，将原始高维属性空间转变为一个低维“子空间”，在这个子空间中样本密度大幅度提高，距离计算也变得更为容易

数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维“嵌入” ，因而可以对数据进行有效的降维

若要求原始空间中样本之间的距离在低维空间中得以保持，即得到**多维缩放**（MDS）



## 10.3 主成分分析（PCA）

在现实任务中，数据可能位于一个高维空间，但实际对学习任务有意义的部分可能只是一个低维的分布。因此，PCA 通过寻找数据的主成分（principal components），将数据从高维空间投影到一个低维子空间中（超平面）。

其主要目标包括：

- 最近重构性：样本点到这个超平面的距离都足够近
- 最大可分性：样本点在这个超平面上的投影能尽可能分开



## 10.4 核化线性降维（KPCA）

PCA 是一种线性降维方法，通过线性变换将高维数据映射到低维空间。然而，当数据的分布具有复杂的非线性关系时，PCA 的线性假设可能无法捕捉数据的真实结构。为了解决这个问题，**核化主成分分析 (Kernelized PCA)** 将传统 PCA 扩展到非线性情况。

核心思想是：
1. 使用核函数 $ k(x_i, x_j)$ 隐式地将原始数据映射到一个高维特征空间 $ \mathcal{F}$ 中
   在高维特征空间中，数据分布可能是线性的，便于使用 PCA。

2. 在特征空间中执行 PCA，而无需显式地计算映射后的高维表示 $ \phi(x)$。这通过核函数来实现。





## 10.5 流形学习

高维空间中的数据实际上分布在一个嵌入高维空间的低维流形上

流形：一个局部与欧氏空间同胚的空间，即在局部具有欧氏空间的性质，能够用欧氏距离描述

如果数据样本在高维空间中看似复杂分布，但在局部区域上仍然具有低维的欧氏性质，就可以在局部建立映射，并将局部映射推广到全局，从而实现降维

### 10.5.1 等度量映射（Isomap）

基于测地线距离的流形学习方法

在高维空间中直接计算两点间的欧氏距离可能会失真。例如，低维流形嵌入到高维空间后，高维空间中的直线距离可能不适用于低维流形的分布，因此需要计算流形上的**测地线距离**。

**多维缩放 (MDS)**：

- 使用多维缩放 (MDS) 方法，将测地线距离矩阵 D 转换为低维嵌入。
- 目标是使得低维嵌入空间中的欧氏距离与测地线距离尽可能一致。



### 10.5.2 局部线性嵌入（LLE）

保留局部几何关系的流形学习方法

在高维空间中，局部邻域内的样本点往往可以用线性组合来近似。LLE 的核心思想是：在高维空间中样本点的局部线性关系在降维后的低维空间中也应保持不变。



## 10.6 度量学习

在机器学习中，高维数据通常需要降维以便于学习和分析。然而，降维的实质是寻找一个更适合学习的低维空间，而低维空间本质上定义了一种新的**距离度量**。度量学习的目标是直接学习一个适合任务的距离度量，以提升学习性能。

给定两个 $d$ 维样本 $x_i $ 和 $x_j $，它们之间的欧氏距离为：
$$
\mathrm{dist}_{\mathrm{ed}}^2(\boldsymbol{x}_i,\boldsymbol{x}_j)=||\boldsymbol{x}_i-\boldsymbol{x}_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\ldots+dist_{ij,d}^2
$$
若考虑不同属性的重要性（权重不同），可以引入权重向量 $w$：
$$
\begin{aligned}
\mathrm{dist}_{\mathrm{wed}}^{2}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})&=||\boldsymbol{x}_{i}-\boldsymbol{x}_{j}||_{2}^{2}=w_{1}\cdot dist_{ij,1}^{2}+w_{2}\cdot dist_{ij,2}^{2}+\ldots+w_{d}\cdot dist_{ij,d}^{2} \\
&=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\mathrm{T}}\mathbf{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})
\end{aligned}
$$
其中，$ \mathbf{W} = \text{diag}(\mathbf{w}) $ 是对角矩阵，且 $ w_k \geq 0 $ 表示第 $k$ 个属性的权重

**马氏距离**

在许多实际问题中，不同属性之间可能存在相关性。此时可以通过一个半正定对称矩阵 $ \mathbf{M} $ 替代权重矩阵 $ \mathbf{W} $：
$$
\mathrm{dist}_{\mathrm{mah}}^2(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i-\boldsymbol{x}_j)^\mathrm{T}\mathbf{M}(\boldsymbol{x}_i-\boldsymbol{x}_j)=\|\boldsymbol{x}_i-\boldsymbol{x}_j\|_\mathbf{M}^2
$$

- 性质：$ \mathbf{M} $ 必须是对称的；$ \mathbf{M} $ 必须是半正定 ($ \mathbf{M} \succeq 0 $)，以确保距离为非负。
  
- 分解形式：$ \mathbf{M} = \mathbf{P}^\top \mathbf{P} $，其中 $ \mathbf{P} $ 是降维矩阵。将数据从高维空间映射到低维空间后，再计算标准欧氏距离。

**度量学习的目标**

学习 $ \mathbf{M} $ 或 $ \mathbf{W} $：通过优化目标函数直接学习 $ \mathbf{M} $ 或 $ \mathbf{W} $，以便定义一个适合当前任务的距离度量。例如：在分类问题中最小化分类错误率；在回归问题中最小化预测误差

**度量学习的实现方法**

**近邻成分分析（NCA）**：通过概率模型优化近邻分类器的性能。

使用**概率投票法**替代传统的多数投票法：给定测试样本 $ x_i $，训练样本 $ x_j $ 对 $ x_i $ 的分类结果影响的概率为：
$$
p_{ij}=\frac{\exp\left(-\|\boldsymbol{x}_i-\boldsymbol{x}_j\|_\mathbf{M}^2\right)}{\sum_l\exp\left(-\|\boldsymbol{x}_i-\boldsymbol{x}_l\|_\mathbf{M}^2\right)}
$$
其中 $ \|x_i - x_j\|^2_{\mathbf{M}} = (x_i - x_j)^\top \mathbf{M} (x_i - x_j) $。

目标函数：最大化留一法准确率，即样本 $ x_i $ 被其邻域内的样本正确分类的概率。样本 $ x_i $ 的留一法正确率为：
$$
p_i = \sum_{j \in \Omega_i} p_{ij}
$$
其中，$ \Omega_i $ 是与 $ x_i $ 同类别的样本集合。

整个样本集上的留一法正确率为：
$$
\sum^mp_i=\sum_{i=1}^m\sum_{j\in\Omega_i}p_{ij}
$$


由 $p_{ij}=\frac{\exp(-\|x_i-x_j\|_{\mathbf{M}}^2)}{\sum_l\exp(-\|x_i-x_l\|_{\mathbf{M}}^2)}$ 和 $\mathbf{M}=\mathbf{PP}^\mathrm{T}$，NCA 的优化目标为
$$
\min_{\mathbf{P}}\quad1-\sum_{i=1}^m\sum_{j\in\Omega_i}\frac{\exp\left(-\|\mathbf{P}^\mathrm{T}\boldsymbol{x}_i-\mathbf{P}^\mathrm{T}\boldsymbol{x}_j\|_2^2\right)}{\sum_l\exp\left(-\|\mathbf{P}^\mathrm{T}\boldsymbol{x}_i-\mathbf{P}^\mathrm{T}\boldsymbol{x}_l\|_2^2\right)}
$$