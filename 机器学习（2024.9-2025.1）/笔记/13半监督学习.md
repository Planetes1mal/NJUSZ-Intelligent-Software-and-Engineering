# 13 半监督学习

## 13.1 未标记样本

有标记样本和未标记样本

主动学习

半监督学习：让学习器不依赖外界交互、 自动地利用未标记样本来提升学习性能

- 纯半监督学习：假定训练数据中的未标记样本并非待预测的数据
- 直推学习：假定学习过程中所考虑的未标记样本恰是待预测数据

未标记样本的假设：

- 聚类假设：假设数据存在簇结构，同一簇的样本属于同一类别
- 流形假设：假设数据分布在一个流形结构上，邻近的样本具有相似的输出值



## 13.2 生成式方法 - 高斯混合模型

假设样本由高斯混合模型生成，且每个类别对应一个高斯混合成分：$p(\boldsymbol{x})=\sum_{i=1}^k\alpha_i\cdot p(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)$，$\alpha_i \geq 0$，$\sum_{i=1}^k \alpha_i=1$

$p(\boldsymbol{x}|\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)=\frac{1}{(2\pi)^{\frac{n}{2}}|\boldsymbol{\Sigma}_i|^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_i)^T\boldsymbol{\Sigma}_i^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_i)}$ 表示样本 $\boldsymbol{x}$ 属于该高斯混合成分的概率

由最大化后验概率可知：
$$
\begin{aligned}f(\boldsymbol{x})&=\arg\max_{j\in\mathcal{Y}}p(y=j\mid\boldsymbol{x})\\&=\arg\max_{j\in\mathcal{Y}}\sum_{i=1}^{N}p(y=j,\Theta=i\mid\boldsymbol{x})\\&=\arg\max_{j\in\mathcal{Y}}\sum_{i=1}^{N}p(y=j\mid\Theta=i,\boldsymbol{x})\cdot p(\Theta=i\mid\boldsymbol{x}) \\
p(\Theta=i\mid\boldsymbol{x})&=\frac{\alpha_i\cdot p(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)}{\sum_{i=1}^N\alpha_i \cdot p(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)}

\end{aligned}
$$
假设样本独立同分布，且由同一个高斯混合模型生成，则 $D_l \cup D_u$ 的对数似然是
$$
\begin{aligned}
\ln p(D_{l}\cup D_{u})&=\sum_{(x_{j},y_{j})\in D_{l}}\ln\left(\sum_{i=1}^{k}\alpha_{i}\cdot p(x_{j}\mid\boldsymbol{\mu}_{i},\boldsymbol{\Sigma}_{i})\cdot p(y_{j}\mid\Theta=i,\boldsymbol{x}_{j})\right)\\
&+\sum_{x_{j}\in D_{u}}\ln\left(\sum_{i=1}^{k}\alpha_{i}\cdot p(x_{j}\mid\boldsymbol{\mu}_{i},\boldsymbol{\Sigma}_{i})\right)
\end{aligned}
$$
高斯混合的参数估计可以采用 EM 算法求解，迭代更新式如下：

- E 步：根据当前模型参数计算未标记样本属于各高斯混合成分的概率
  $$
  \gamma_{ji}=\frac{\alpha_i\cdot p(\boldsymbol{x}_j|\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)}{\sum_{i=1}^N\alpha_i\cdot p(\boldsymbol{x}_j|\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)}
  $$

- M 步：基于 $\gamma_{ji}$ 更新模型参数
  $$
  \begin{aligned}
  \boldsymbol{\mu}_{i}&=\frac{1}{\sum_{\boldsymbol{x}_{j}\in D_{u}}\gamma_{ji}+l_{i}}\left(\sum_{\boldsymbol{x}_{j}\in D_{u}}\gamma_{ji}\boldsymbol{x}_{j}+\sum_{(\boldsymbol{x}_{j},y_{j})\in D_{l}\wedge y_{j}=i}\boldsymbol{x}_{j}\right) \\
  \boldsymbol{\Sigma}_{i}&=\frac{1}{\sum_{\boldsymbol{x}_{j}\in D_{u}}\gamma_{ji}+l_{i}}\left( \sum_{\boldsymbol{x}_{j}\in D_{u}}\gamma_{ji}(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i})(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i})^{\mathrm{T}} 
  + \sum_{(\boldsymbol{x}_j,y_j)\in D_l\wedge y_j=i}(\boldsymbol{x}_j-\boldsymbol{\mu}_i)(\boldsymbol{x}_j-\boldsymbol{\mu}_i)^\mathrm{T} \right) \\
  \alpha_{i}&=\frac1m\left(\sum_{\boldsymbol{x}_j\in D_u}\gamma_{ji}+l_i\right)
  \end{aligned}
  $$



## 13.3 半监督 SVM - TSVM

TSVM 采用局部搜索来迭代地寻找近似解：先利用有标记样本学得一个 SVM，利用这个 SVM 对未标记数据进行标记指派（将 SVM 预测的结果作为 伪标记赋予未标记样本），接下来，TSVM 找出两个标记指派为异类且很可能发生错误的未标记样本，交换它们的标记，再求解
$$
\begin{aligned}
\operatorname*{min}_{\boldsymbol{w},b,\hat{\boldsymbol{y}},\boldsymbol{\xi}} \quad &\frac{1}{2}\|\boldsymbol{w}\|_{2}^{2}+C_{l}\sum_{i=1}^{l}\xi_{i}+C_{u}\sum_{i=l+1}^{m}\xi_{i} \\
\mathbf{s.t.} \quad &y_i(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b)\geqslant1-\xi_i,i=1,2,\ldots,l \\
&\hat{y}_i(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b)\geqslant1-\xi_i,i=l+1,l+2,\ldots,m \\
&\xi_{i}\geqslant0,i=1,2,\ldots,m
\end{aligned}
$$


## 13.4 图半监督学习

给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高（或相关性很强），则对应的结点之间存在一条边，边的“强度”（strength）正比于样本之间的相似度（或相关性）



## 13.5 基于分歧的方法 - 协同训练

基于分歧的方法使用多学习器

协同训练：利用了多视图的“相容互补性”

若两个视图充分且条件独立，则可利用未标记样本通过协同训练将弱分类器的泛化性能提升到任意高



## 13.6 半监督聚类 - 约束 k 均值

聚类任务中获得的监督信息大致有两种类型：

1. 必连与勿连约束，前者是指样本必属于同一个簇，后者则是指样本必不属于同一个簇
2. 假设少量有标记样本属于 k 个聚类簇



