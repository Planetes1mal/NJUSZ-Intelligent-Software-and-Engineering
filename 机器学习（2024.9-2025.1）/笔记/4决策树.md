# 4 决策树

## 4.1 基本流程

决策树模型：

- 决策树的组成和决策方式
  - 决策树基于“树”结构进行决策：
    - 每个“内部结点”对应于某个属性上的“测试”（test）。
    - 每个分支对应于该测试的一种可能结果（即该属性的某个取值）。
    - 每个“叶结点”对应于一个“预测结果”。

- 学习和预测过程
  - 学习过程：决策树的学习通过对训练样本的分析来确定“划分属性”，即内部结点所对应的属性。
  - 预测过程：对测试样例，从根结点开始，沿着划分属性构成的“判定测试序列”下行，直到叶结点，输出最终的预测结果。

决策树的构建遵循“分而治之”（divide-and-conquer）的策略，自根至叶递归生成决策树。在每个中间结点寻找一个“划分”（split or test）属性。对划分后的子结点，递归执行相同操作，直到满足停止条件。

决策树生成过程中存在以下三种停止条件：

1. **当前结点包含的样本全属于同一类别**，无需划分。
2. **当前属性集为空**，或者所有样本在所有属性上取值相同，无法划分。
3. **当前结点包含的样本集合为空**，不能划分。



## 4.2 划分选择

决策树学习的关键在于如何选择最优划分属性。

随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高

经典的属性划分方法：信息增益、增益率、基尼指数

### 4.2.1 信息增益

信息熵：度量样本集合“纯度”的指标

假定当前样本集合 D 中第 k 类样本所占的比例为 $p_k$，则 D 的信息熵定义
$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k
$$
$\operatorname{Ent}(D)$ 值越小，D 纯度越高，$\operatorname{Ent}(D)$ 最小值为0，最大值为 $\log_2|\mathcal{Y}|$

信息增益直接以信息熵为基础，计算当前划分对信息熵所造成的变化

离散属性 a 的取值：$\{a^1,a^2,\ldots,a^V\}$，$D_v$ 是 D 中在 a上取值为 $a_v$ 的样本集合

以属性 a 对数据集 D 进行划分所获得的信息增益为：
$$
\mathrm{Gain}(D,a)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)
$$
信息增益越大，使用属性 a 来进行划分所获得的“纯度提升”越大

### 4.2.2 增益率

信息增益对可取值数目较多的属性有所偏好

增益率定义：
$$
\text{Gain\_ratio} (D,a)=\frac{\mathrm{Gain}(D,a)}{\mathrm{IV}(a)}
$$
其中，属性固有值定义：$\mathrm{IV}(a)=-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}\operatorname{log}_{2}\frac{|D^{v}|}{|D|}$

属性 a 的可能取值数目越多（即 V 越大），则 $\mathrm{IV}(a)$ 的值通常就越大

启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的

### 4.2.3 基尼指数

$$
\begin{aligned}\mathrm{Gini}(D)=\sum_{k=1}^{|\mathcal{Y}|}\sum_{k^{\prime}\neq k}p_{k}p_{k^{\prime}}=1-\sum_{k=1}^{|\mathcal{Y}|}p_{k}^{2}\end{aligned}
$$

反映了从 D 中随机抽取两个样例，其类别标记不一致的概率

Gini(D) 越小，数据集 D 的纯度越高

属性 a 的基尼指数：$\mathrm{Gini}\_\mathrm{index}(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Gini}(D^v)$

在候选属性集合中，选取那个使划分后基尼指数最小的属性：$a_*=\underset{a\in A}{\operatorname*{\text{argmin Gini index}}}(D,a)$



## 4.3 剪枝处理

为了尽可能正确分类训练样本，有可能造成分支过多，即过拟合

剪枝是决策树对付“过拟合”的主要手段

剪枝的基本策略：

- 预剪枝（pre-pruning）：提前终止某些分支的生长

- 后剪枝（post-pruning）：生成一棵完全树，再“回头”剪枝

剪枝过程中需评估剪枝前后决策树的优劣

### 4.3.1 预剪枝

决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别

针对上述数据集，基于信息增益准则，选取属性“脐部”划分训练集。分别计算划分前（即直接将该结点作为叶结点）及划分后的验证集精度，判断是否需要划分。若划分后能提高验证集精度，则划分，对划分后的属性，执行同样判断；否则，不划分

优点：降低过拟合风险；显著减少训练时间和测试时间开销

缺点：欠拟合风险：有些分支的当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分却有可能导致性能显著提高。预剪枝基于“贪心”本质禁止这些分支展开，带来了欠拟合风险

### 4.3.2 后剪枝

先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点

优点：后剪枝比预剪枝保留了更多的分支，欠拟合风险小，泛化性能往往优于预剪枝决策树

缺点：训练时间开销大：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察

预剪枝 vs. 后剪枝：

- 时间开销：
  - 预剪枝：测试时间开销降低，训练时间开销降低
  - 后剪枝：测试时间开销降低，训练时间开销增加
- 过/欠拟合风险：
  - 预剪枝：过拟合风险降低，欠拟合风险增加
  - 后剪枝：过拟合风险降低，欠拟合风险基本不变
- 泛化性能：后剪枝通常优于预剪枝



## 4.4 连续与缺失值

### 4.4.1 连续值处理

连续属性离散化、二分法

连续属性 $a$ 出现 $n$ 个不同的取值，把区间 $\left[a^i,a^{i+1}\right)$ 的中位点 $\frac {ai+a^{i+1}}{2}$ 作为候选划分点

$$T_a=\left\{\frac{a^i+a^{i+1}}2\mid1\leq i\leq n-1\right\}$$



假定连续属性 $a$ 在样本集 $D$ 上出现 $n$ 个不同的取值，从小到大排列为 $\{a_1, a_2, \dots, a_n\}$，基于划分点将 $D$ 分为两个子集 $D_1$ 和 $D_2$，$D_1$ 包含在属性 $a$ 上取值不大于划分点的样本，$D_2$ 包含在属性 $a$ 上取值大于划分点的样本，这样可以生成包含 $n-1$ 个元素的候选划分点集合。

采用离散属性的划分方法，考察这些划分点的效果。对每个候选划分点计算其信息增益 $\mathrm{Gain}(D,a,t)$。选择使信息增益最大化的划分点进行样本集合的划分。
$$
\begin{aligned}\operatorname{Gain}(D,a)&=\max_{t\in T_{a}}\mathrm{Gain}(D,a,t)\\&=\max_{t\in T_{a}}\mathrm{Ent}(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_{t}^{\lambda}|}{|D|}\mathrm{Ent}(D_{t}^{\lambda})\end{aligned}
$$

### 4.4.2 缺失值处理

属性值“缺失”，如何进行划分属性选择？给定划分属性，若样本在该属性上的值缺失，如何进行划分？

基本思路：对样本赋予权重，并通过权重划分来处理缺失值

Q1：如何在属性缺失的情况下进行划分属性选择？

- $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集，$\tilde{D}^v$ 表示 $\tilde{D}$ 中在属性 $a$ 上取值为 $a^v$ 的样本子集，$\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 $k$ 类的样本子集为每个样本 $x$ 赋予一个权重 $w_x$，并定义：

- 无缺失值样本所占的比例
  $$
  
  \rho=\frac{\sum_{\boldsymbol{x}\in\tilde{D}}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in D}w_{\boldsymbol{x}}}
  $$

- 无缺失值样本中第 k 类所占比例
  $$
  \tilde{p}_{k}=\frac{\sum_{\boldsymbol{x}\in\tilde{D}_k}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in\tilde{D}}w_{\boldsymbol{x}}}
  $$

- 无缺失值样本中在属性 $a$ 上取值为 $a^v$ 的样本所占比例
  $$
  \tilde{r}_{v}=\frac{\sum_{\boldsymbol{x}\in\tilde{D}^v}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in\tilde{D}}w_{\boldsymbol{x}}}
  $$

- 基于上述定义，属性 $a$ 的信息增益公式修正为：
  $$
  \begin{aligned}
  \operatorname{Gain}(D,a)&=\rho\times\operatorname{Gain}(\tilde{D},a)=\rho\times\left(\operatorname{Ent}\left(\tilde{D}\right)-\sum_{v=1}^{V}\tilde{r}_{v}\operatorname{Ent}\left(\tilde{D}^{v}\right)\right)
  \\
  \operatorname{Ent}(\tilde{D})&=-\sum_{k=1}^{|\mathcal{Y}|}\tilde{p}_k\log_2\tilde{p}_k
  \end{aligned}
  $$

Q2：若样本在划分属性上的值缺失，如何进行划分？

- 若样本 $x$ 在划分属性 $a$ 上的取值已知，则将 $x$ 划入与其取值对应的子结点，同时样本权值在子结点中保持不变 $w_x$。  
- 若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子结点，且在每个子结点中调整样本权值为：$\tilde{r}_{v}\cdot w_{x}$。直观解释：这相当于让同一个样本以不同的概率划入不同的子结点。



## 4.5 多变量决策树

单变量决策树分类边界：轴平行

多变量决策树：

- 非叶节点不再是仅对某个属性，而是对属性的线性组合
- 每个非叶结点是一个形如 $\sum_{i=1}^dw_ia_i=t$ 的线性分类器，其中 $w_i$ 是属性 $a_i$ 权值，$w_i$ 和 t 可在该结点所含的样本集和属性集上学得