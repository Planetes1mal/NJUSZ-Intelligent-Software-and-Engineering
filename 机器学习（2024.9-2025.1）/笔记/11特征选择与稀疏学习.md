# 11 特征选择与稀疏学习

## 11.1 子集搜索与评价

特征是描述物体的属性，其所包含信息能由其他特征推演出来

特征的分类：相关特征（对当前学习任务有用的属性）、无关特征、冗余特征

特征选择：选择当前任务相关特征

产生初始候选子→评价候选子集的好坏→基于评价结果产生下一个候选子集

**子集搜索**：用贪心策略选择包含重要信息的特征子集

- 前向搜索：逐渐增加相关特征
- 后向搜索：从完整的特征集合开始，逐渐减少特征
- 双向搜索：每一轮逐渐增加相关特征，同时减少无关特征

**子集评价**：

- 特征子集确定了对数据集的一个划分，每个划分区域对应着特征子集上的某种取值
- 与样本标记对应的划分的差异越小，则说明当前特征子集越好
- 用信息熵进行子集评价
  - 特征子集 A 确定了对数据集 D 的一个划分
  - A 上的取值将数据集 D 分为 V 份，每一份用 $D^v$表示。D 上的信息熵定义为$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k$，第 k 类样本所占比例为 $p_k$
  - 特征子集𝐴的信息增益为：$\mathrm{Gain}(A)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)$

常见的特征选择方法：过滤式、包裹式、嵌入式



## 11.2 过滤式选择 - Relief 方法

先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型；特征选择过程与后续学习器无关

定义相关统计量：为每个初始特征分配一个“相关统计量”，用于度量特征的重要性。相关统计量越大，说明该特征在区分不同类别上越有用。

计算相关统计量：

- 猜中近邻：$x_i$的同类样本中的最近邻 $x_{i,nh}$

- 猜错近邻：$x_i$的异类样本中的最近邻 $x_{i,nm}$

相关统计量对应于属性 j 的分量为
$$
\delta^j=\sum_i-\mathrm{diff}(x_i^j,x_{i,\mathrm{nh}}^j)^2+\mathrm{diff}(x_i^j,x_{i,\mathrm{nm}}^j)^2
$$
其中，$\text{diff}(x_i^{j}, x_k^{j})$表示样本 $x_i $和 $x_k $在特征 $j $上的差异：

- 对于离散型特征：
  $$
  \text{diff}(x_i^{j}, x_k^{j}) =
  \begin{cases}
  0 & \text{若 } x_i^{j} = x_k^{j}, \\
  1 & \text{若 } x_i^{j} \neq x_k^{j}.
  \end{cases}
  $$
  
- 对于连续型特征：
  $$
  \text{diff}(x_i^{j}, x_k^{j}) = |x_i^{j} - x_k^{j}|
  $$
  

相关统计量越大，属性 j 上，猜中近邻比猜错近邻越近，即属性 j 对区分对错越有用



## 11.3 包裹式选择 - LVW 

直接把最终将要使用的学习器的性能作为特征子集的评价准则

LVM 基本步骤

- 在循环的每一轮随机产生一个特征子集
- 在随机产生的特征子集上通过交叉验证推断当前特征子集的误差
- 进行多次循环，在多个随机产生的特征子集中选择误差最小的特征子集作为最终解



## 11.4 嵌入式选择与 $L_1$正则化

将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，在学习器训练过程中自动地进行特征选择

**正则化**是嵌入式选择的关键技术，通过向损失函数添加正则化项，限制模型复杂度，防止过拟合

**1. $L_2$正则化（岭回归）**

最常见的正则化方法之一是 $L_2$正则化，其优化目标为：
$$
\min_\boldsymbol{w}\sum_{i=1}^m(y_i-\boldsymbol{w}^\top\boldsymbol{x}_i)^2+\lambda\|\boldsymbol{w}\|_2^2
$$
其中：$\|\mathbf{w}\|_2^2 = \sum_j w_j^2$是 $L_2$范数；$\lambda > 0$是正则化强度，控制参数的收缩程度。

$L_2$正则化倾向于生成较小的参数，但一般不会将参数缩至 $0$

**2. $L_1$正则化（LASSO回归）**

与 $L_2$正则化不同，$L_1$正则化通过引入稀疏性，使部分特征的权重被压缩为 $0$，从而实现特征选择。其优化目标为：

$$
\min_{\boldsymbol{w}}\sum_{i=1}^m(y_i-\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i)^2+\lambda\|\boldsymbol{w}\|_1
$$
其中：$\|\mathbf{w}\|_1 = \sum_j |w_j|$是 $L_1$范数。

相比 $L_2$正则化，$L_1$正则化更倾向于将部分参数缩至 $0$，从而实现自动特征选择

**$L_1$正则化的求解**：**近端梯度下降**



## 11.5 稀疏表示与字典学习

将数据集看成一个矩阵，每行对应一个样本，每列对应一个特征

稀疏表示的优势：文本数据线性可分；存储高效

**字典学习**：为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示

给定数据集 $\{x_1,x_2,\ldots,\boldsymbol{x}_m\},\boldsymbol{x}_i\in\mathbb{R}^d$，学习目标是字典矩阵 $B\in\mathbb{R}^d\times k$以及样本的稀疏表示 $\alpha_i\in\mathbb{R}^k$，$k$称为字典的词汇量，通常由用户指定

则最简单的字典学习的优化形式为
$$
\underset{\mathbf{B},\alpha_i}{\operatorname*{min}}\sum_{i=1}^m\|\boldsymbol{x}_i-\mathbf{B}\boldsymbol{\alpha}_i\|_2^2+\lambda\sum_{i=1}^m\|\boldsymbol{\alpha}_i\|_1
$$
**字典学习的解法**



## 11.6 压缩感知

利用接收到的压缩、丢包后的数字信号，精确重构出原信号

假设信号 $x \in \mathbb{R}^m$ 是长度为 $m$ 的离散信号，采样矩阵 $\Phi \in \mathbb{R}^{n \times m}$ 满足 $n \ll m$。采样得到的信号为：$y=\Phi x$

其中：$y \in \mathbb{R}^n$ 是采样信号；$\Phi$ 是观测矩阵

由于 $n \ll m$，直接从 $y$ 中恢复 $x$ 是欠定问题，但如果 $x$ 是稀疏的，就可以通过压缩感知技术近乎完美地恢复 $x$。

若存在某个变换矩阵 $\Psi$，使得信号 $x$ 可以表示为：$x = \Psi s$，其中 $s$ 是稀疏向量，则有：$y = \Phi \Psi \mathbf{s} = \mathbf{A} s$，$\mathbf{A}$ 具有“限定等距性”时，可以近乎完美地恢复 $s$ 

**限定等距性 (RIP)**
$$
(1 - \delta_k) \|s\|_2^2 \leq \|\mathbf{A}_k \mathbf{s}\|_2^2 \leq (1 + \delta_k) \|s\|_2^2
$$
**矩阵补全**：从部分观测到的矩阵元素中恢复出完整矩阵

